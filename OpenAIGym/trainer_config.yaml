default:
    trainer: DQN            # trainer type
    batch_size: 1024        # batch size
    memory_size: 1024       # allocated memory size
    beta: 5.0e-3
    epsilon: 0.2            # exploration rate
    epsilon_min: 0.01       # minimal exploration rate
    epsilon_decay: 0.999
    alpha: 0.1              # learning factor
    alpha_min: 0.01         # learning factor min
    alpha_decay: 0.995      # learning factor decay
    loss_clipping: 0.2      # Only implemented clipping for the surrogate loss, paper said it was best
    entropy_loss: 0.001     # Entropy loss
    exploration_noise: 0.2  # Exploration noise
    critic_hidden_units: 256 # number of hidden unit in the hidden layer
    gamma: 0.99             # discount factor
    tau: .125               # soft target model updates
    num_layers: 2           # number of hidden layers
    hidden_units: 128       # number of hidden unit in the hidden layer
    use_recurrent: false    # use R-NN
    lambd: 0.95
    learning_rate: 0.001    # learning rate
    max_steps: 10000       # maximum number of state for an episode
    normalize: false        # add nomalization layer to the associated NN
    num_epoch: 5000         # total number of epochs
    time_horizon: 64        # time horizon
    sequence_length: 64     # 
    summary_freq: 1000      # Frequency at which at summary will be printed out
    buffer_size: 10240

MCD_DQN_LSTM:
    trainer: DQN_LSTM       # trainer type
    batch_size: 512         # batch size
    memory_size: 512        # allocated memory size
    beta: 5.0e-3
    alpha: 0.2              # learning factor
    alpha_min: 0.01         # learning factor min
    alpha_decay: 0.995      # learning factor decay
    epsilon: 0.3            # exploration rate
    epsilon_min: 0.1        # minimal exploration rate
    epsilon_decay: 0.999
    gamma: 0.99             # discount factor
    tau: .001               # soft target model updates
    num_layers: 1           # number of hidden layers
    hidden_units: 32        # number of hidden unit in the hidden layer
    use_recurrent: false    # use R-NN
    lambd: 0.95
    learning_rate: 0.001    # learning rate or learning factor
    max_steps: 5000000      # maximum number of state for an episode
    normalize: false        # add nomalization layer to the associated NN
    num_epoch: 10000        # total number of epochs
    time_horizon: 64        # time horizon
    sequence_length: 64     # 
    summary_freq: 1000      # Frequency at which at summary will be printed out

MCD_DQN_RC:
    trainer: DQN_RC         # trainer type
    batch_size: 512         # batch size
    memory_size: 512       # allocated memory size
    beta: 5.0e-3
    alpha: 0.2              # learning factor
    alpha_min: 0.01         # learning factor min
    alpha_decay: 0.995      # learning factor decay
    epsilon: 0.3            # exploration rate
    epsilon_min: 0.1        # minimal exploration rate
    epsilon_decay: 0.999
    gamma: 0.99             # discount factor
    tau: .001               # soft target model updates
    num_layers: 2           # number of hidden layers
    hidden_units: 64        # number of hidden unit in the hidden layer
    use_recurrent: false    # use R-NN
    lambd: 0.95
    learning_rate: 0.001    # learning rate or learning factor
    max_steps: 5000000      # maximum number of state for an episode
    normalize: false        # add normalization layer to the associated NN
    num_epoch: 10000        # total number of epochs
    time_horizon: 64        # time horizon
    sequence_length: 64     # 
    summary_freq: 1000      # Frequency at which at summary will be printed out

MCD_DQN:
    trainer: DQN            # trainer type
    batch_size: 512         # batch size
    memory_size: 10240      # allocated memory size
    beta: 5.0e-3
    alpha: 0.2              # learning factor
    alpha_min: 0.01         # learning factor min
    alpha_decay: 0.995      # learning factor decay
    epsilon: 0.2            # exploration rate
    epsilon_min: 0.01       # minimal exploration rate
    epsilon_decay: 0.999
    gamma: 0.99             # discount factor
    tau: .001               # soft target model updates
    num_layers: 2           # number of hidden layers
    hidden_units: 32        # number of hidden unit in the hidden layer
    use_recurrent: false    # use R-NN
    lambd: 0.95
    learning_rate: 0.001    # learning rate
    max_steps: 5000000      # maximum number of state for an episode
    normalize: false        # add normalization layer to the associated NN
    num_epoch: 10000        # total number of epochs
    time_horizon: 64        # time horizon
    sequence_length: 64     # 
    summary_freq: 1000      # Frequency at which at summary will be printed out

MCD_SARSA:
    trainer: SARSA          # trainer type
    batch_size: 512         # batch size
    memory_size: 5120       # allocated memory size
    alpha: 0.2              # learning factor
    alpha_min: 0.01         # learning factor min
    alpha_decay: 0.995      # learning factor decay
    epsilon: 0.1            # exploration rate
    epsilon_min: 0.01       # minimal exploration rate
    epsilon_decay: 0.999
    gamma: 0.99             # discount factor
    max_steps: 5000000      # maximum number of state for an episode
    num_epoch: 10000        # total number of epochs
    summary_freq: 1000      # Frequency at which at summary will be printed out

MCD_PPO:
    trainer: PPO           # trainer type
    batch_size: 512        # batch size
    memory_size: 10240      # allocated memory size
    beta: 5.0e-3
    gamma: 0.99             # discount factor
    loss_clipping: 0.2      # Only implemented clipping for the surrogate loss, paper said it was best
    entropy_loss: 0.001     # Entropy loss
    exploration_noise: 0.2  # Exploration noise
    num_layers: 2           # number of hidden layers
    hidden_units: 64        # number of hidden unit in the hidden layer
    use_recurrent: false    # use R-NN
    lambd: 0.95
    learning_rate: 0.001    # learning rate or learning factor
    max_steps: 5000         # maximum number of state for an episode
    normalize: false        # add normalization layer to the associated NN
    num_epoch: 10000        # total number of epochs
    time_horizon: 64        # time horizon
    sequence_length: 64     # 
    summary_freq: 1000      # Frequency at which at summary will be printed out

MCD_DDPG:
    trainer: DDPG           # trainer type
    batch_size: 512        # batch size
    memory_size: 10240      # allocated memory size
    beta: 5.0e-3
    alpha: 0.2              # learning factor
    alpha_min: 0.01         # learning factor min
    alpha_decay: 0.995      # learning factor decay
    epsilon: 0.3            # exploration rate
    epsilon_min: 0.1        # minimal exploration rate
    epsilon_decay: 0.999
    gamma: 0.99             # discount factor
    tau: .001                # soft target model updates
    num_layers: 2           # number of hidden layers
    hidden_units: 64        # number of hidden unit in the hidden layer
    use_recurrent: false    # use R-NN
    lambd: 0.95
    learning_rate: 0.001    # learning rate or learning factor
    max_steps: 5000000      # maximum number of state for an episode
    normalize: false        # add normalization layer to the associated NN
    num_epoch: 10000        # total number of epochs
    time_horizon: 64        # time horizon
    sequence_length: 64     # 
    summary_freq: 1000      # Frequency at which at summary will be printed out

MCD_DDPGCat:
    trainer: DDPGCat           # trainer type
    batch_size: 512        # batch size
    memory_size: 10240      # allocated memory size
    beta: 5.0e-3
    alpha: 0.2              # learning factor
    alpha_min: 0.01         # learning factor min
    alpha_decay: 0.995      # learning factor decay
    epsilon: 0.3            # exploration rate
    epsilon_min: 0.1        # minimal exploration rate
    epsilon_decay: 0.999
    gamma: 0.99             # discount factor
    tau: .001                # soft target model updates
    num_layers: 2           # number of hidden layers
    hidden_units: 64        # number of hidden unit in the hidden layer
    use_recurrent: false    # use R-NN
    lambd: 0.95
    learning_rate: 0.001    # learning rate or learning factor
    max_steps: 5000000      # maximum number of state for an episode
    normalize: false        # add normalization layer to the associated NN
    num_epoch: 10000        # total number of epochs
    time_horizon: 64        # time horizon
    sequence_length: 64     # 
    summary_freq: 1000      # Frequency at which at summary will be printed out

MCD_A2C:
    trainer: A2C           # trainer type
    batch_size: 512        # batch size
    memory_size: 512       # allocated memory size
    gamma: 0.99             # discount factor
    num_layers: 2           # number of hidden layers
    hidden_units: 64        # number of hidden unit in the hidden layer
    use_recurrent: false    # use R-NN
    learning_rate: 0.001    # learning rate or learning factor
    max_steps: 5000000      # maximum number of state for an episode
    normalize: false        # add normalization layer to the associated NN
    num_epoch: 10000        # total number of epochs
    summary_freq: 1000      # Frequency at which at summary will be printed out

MCD_MADDPG:
    trainer: MADDPG           # trainer type
    agent_config_path: C:\Users\jubakakeu\Documents\GitHub\Multi-Agent-RL-Energy-Management\OpenAIGym\MARL_config.yaml
    p_min: 0
    p_max: 100
    p_slope: 10
    batch_size: 512        # batch size
    memory_size: 10240      # allocated memory size
    beta: 5.0e-3
    alpha: 0.2              # learning factor
    alpha_min: 0.01         # learning factor min
    alpha_decay: 0.995      # learning factor decay
    epsilon: 0.3            # exploration rate
    epsilon_min: 0.1        # minimal exploration rate
    epsilon_decay: 0.999
    gamma: 0.99             # discount factor
    tau: .001                # soft target model updates
    num_layers: 2           # number of hidden layers
    hidden_units: 64        # number of hidden unit in the hidden layer
    use_recurrent: false    # use R-NN
    lambd: 0.95
    learning_rate: 0.001    # learning rate or learning factor
    max_steps: 5000000      # maximum number of state for an episode
    normalize: false        # add normalization layer to the associated NN
    num_epoch: 10000        # total number of epochs
    time_horizon: 64        # time horizon
    sequence_length: 64     # 
    summary_freq: 1000      # Frequency at which at summary will be printed out

MCD_MAPPO:
    trainer: MAPPO          # trainer type
    agent_config_path: C:\Users\Administrator\Desktop\SP347\02_Paper\2019_Multi_Agent_Energy_Management\03_Pythom_Workspace\OpenAIGym\MARL_config.yaml
    batch_size: 512        # batch size
    memory_size: 10240      # allocated memory size
    beta: 5.0e-3
    loss_clipping: 0.2      # Only implemented clipping for the surrogate loss, paper said it was best
    entropy_loss: 0.001     # Entropy loss
    exploration_noise: 0.2  # Exploration noise
    gamma: 0.99             # discount factor
    num_layers: 2           # number of hidden layers
    hidden_units: 64        # number of hidden unit in the hidden layer
    critic_hidden_units: 256 # number of hidden unit in the hidden layer
    use_recurrent: false    # use R-NN
    lambd: 0.95
    learning_rate: 0.001    # learning rate or learning factor
    max_steps: 5000000      # maximum number of state for an episode
    normalize: false        # add normalization layer to the associated NN
    num_epoch: 10000        # total number of epochs
    time_horizon: 64        # time horizon
    sequence_length: 64     # 
    summary_freq: 1000      # Frequency at which at summary will be printed out