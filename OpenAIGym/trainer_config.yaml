default:
    trainer: DQN            # trainer type
    batch_size: 1024        # batch size
    memory_size: 1024       # allocated memory size
    beta: 5.0e-3
    epsilon: 0.2            # exploration rate
    epsilon_min: 0.01       # minimal exploration rate
    epsilon_decay: 0.999
    alpha: 0.1              # learning factor
    alpha_min: 0.01         # learning factor min
    alpha_decay: 0.995      # learning factor decay
    loss_clipping: 0.2      # Only implemented clipping for the surrogate loss, paper said it was best
    entropy_loss: 0.001     # Entropy loss
    exploration_noise: 0.2  # Exploration noise
    critic_hidden_units: 256 # number of hidden unit in the hidden layer
    gamma: 0.99             # discount factor
    tau: .125               # soft target model updates
    num_layers: 2           # number of hidden layers
    hidden_units: 128       # number of hidden unit in the hidden layer
    use_recurrent: false    # use R-NN
    lambd: 0.95
    learning_rate: 0.001    # learning rate
    max_steps: 10000       # maximum number of state for an episode
    normalize: false        # add nomalization layer to the associated NN
    num_epoch: 5000         # total number of epochs
    time_horizon: 64        # time horizon
    sequence_length: 64     # 
    summary_freq: 1000      # Frequency at which at summary will be printed out
    buffer_size: 10240

MCD_DQN_LSTM:
    trainer: DQN_LSTM       # trainer type
    batch_size: 512         # batch size
    memory_size: 512        # allocated memory size
    beta: 5.0e-3
    alpha: 0.2              # learning factor
    alpha_min: 0.01         # learning factor min
    alpha_decay: 0.995      # learning factor decay
    epsilon: 0.3            # exploration rate
    epsilon_min: 0.1        # minimal exploration rate
    epsilon_decay: 0.999
    gamma: 0.99             # discount factor
    tau: .001               # soft target model updates
    num_layers: 1           # number of hidden layers
    hidden_units: 32        # number of hidden unit in the hidden layer
    use_recurrent: false    # use R-NN
    lambd: 0.95
    learning_rate: 0.001    # learning rate or learning factor
    max_steps: 5000000      # maximum number of state for an episode
    normalize: false        # add nomalization layer to the associated NN
    num_epoch: 10000        # total number of epochs
    time_horizon: 64        # time horizon
    sequence_length: 64     # 
    summary_freq: 1000      # Frequency at which at summary will be printed out

MCD_DQN_RC:
    trainer: DQN_RC         # trainer type
    batch_size: 512         # batch size
    memory_size: 512       # allocated memory size
    beta: 5.0e-3
    alpha: 0.2              # learning factor
    alpha_min: 0.01         # learning factor min
    alpha_decay: 0.995      # learning factor decay
    epsilon: 0.3            # exploration rate
    epsilon_min: 0.1        # minimal exploration rate
    epsilon_decay: 0.999
    gamma: 0.99             # discount factor
    tau: .001               # soft target model updates
    num_layers: 2           # number of hidden layers
    hidden_units: 64        # number of hidden unit in the hidden layer
    use_recurrent: false    # use R-NN
    lambd: 0.95
    learning_rate: 0.001    # learning rate or learning factor
    max_steps: 5000000      # maximum number of state for an episode
    normalize: false        # add normalization layer to the associated NN
    num_epoch: 10000        # total number of epochs
    time_horizon: 64        # time horizon
    sequence_length: 64     # 
    summary_freq: 1000      # Frequency at which at summary will be printed out

MCD_DQN:
    trainer: DQN            # trainer type
    batch_size: 512         # batch size
    memory_size: 10240      # allocated memory size
    beta: 5.0e-3
    alpha: 0.2              # learning factor
    alpha_min: 0.01         # learning factor min
    alpha_decay: 0.995      # learning factor decay
    epsilon: 0.2            # exploration rate
    epsilon_min: 0.01       # minimal exploration rate
    epsilon_decay: 0.999
    gamma: 0.99             # discount factor
    tau: .001               # soft target model updates
    num_layers: 2           # number of hidden layers
    hidden_units: 32        # number of hidden unit in the hidden layer
    use_recurrent: false    # use R-NN
    lambd: 0.95
    learning_rate: 0.001    # learning rate
    max_steps: 5000000      # maximum number of state for an episode
    normalize: false        # add normalization layer to the associated NN
    num_epoch: 10000        # total number of epochs
    time_horizon: 64        # time horizon
    sequence_length: 64     # 
    summary_freq: 1000      # Frequency at which at summary will be printed out

MCD_SARSA:
    trainer: SARSA          # trainer type
    batch_size: 512         # batch size
    memory_size: 5120       # allocated memory size
    alpha: 0.2              # learning factor
    alpha_min: 0.01         # learning factor min
    alpha_decay: 0.995      # learning factor decay
    epsilon: 0.1            # exploration rate
    epsilon_min: 0.01       # minimal exploration rate
    epsilon_decay: 0.999
    gamma: 0.99             # discount factor
    max_steps: 5000000      # maximum number of state for an episode
    num_epoch: 10000        # total number of epochs
    summary_freq: 1000      # Frequency at which at summary will be printed out

MCD_PPO:
    trainer: PPO           # trainer type
    batch_size: 512        # batch size
    memory_size: 10240      # allocated memory size
    beta: 5.0e-3
    gamma: 0.99             # discount factor
    loss_clipping: 0.2      # Only implemented clipping for the surrogate loss, paper said it was best
    entropy_loss: 0.001     # Entropy loss
    exploration_noise: 0.2  # Exploration noise
    num_layers: 2           # number of hidden layers
    hidden_units: 64        # number of hidden unit in the hidden layer
    use_recurrent: false    # use R-NN
    lambd: 0.95
    learning_rate: 0.001    # learning rate or learning factor
    max_steps: 5000         # maximum number of state for an episode
    normalize: false        # add normalization layer to the associated NN
    num_epoch: 10000        # total number of epochs
    time_horizon: 64        # time horizon
    sequence_length: 64     # 
    summary_freq: 1000      # Frequency at which at summary will be printed out

MCD_DDPG:
    trainer: DDPG           # trainer type
    batch_size: 512        # batch size
    memory_size: 10240      # allocated memory size
    beta: 5.0e-3
    alpha: 0.2              # learning factor
    alpha_min: 0.01         # learning factor min
    alpha_decay: 0.995      # learning factor decay
    epsilon: 0.3            # exploration rate
    epsilon_min: 0.1        # minimal exploration rate
    epsilon_decay: 0.999
    gamma: 0.99             # discount factor
    tau: .001                # soft target model updates
    num_layers: 2           # number of hidden layers
    hidden_units: 64        # number of hidden unit in the hidden layer
    use_recurrent: false    # use R-NN
    lambd: 0.95
    learning_rate: 0.001    # learning rate or learning factor
    max_steps: 5000000      # maximum number of state for an episode
    normalize: false        # add normalization layer to the associated NN
    num_epoch: 10000        # total number of epochs
    time_horizon: 64        # time horizon
    sequence_length: 64     # 
    summary_freq: 1000      # Frequency at which at summary will be printed out

MCD_DDPGCat:
    trainer: DDPGCat           # trainer type
    batch_size: 512        # batch size
    memory_size: 10240      # allocated memory size
    beta: 5.0e-3
    alpha: 0.2              # learning factor
    alpha_min: 0.01         # learning factor min
    alpha_decay: 0.995      # learning factor decay
    epsilon: 0.3            # exploration rate
    epsilon_min: 0.1        # minimal exploration rate
    epsilon_decay: 0.999
    gamma: 0.99             # discount factor
    tau: .001                # soft target model updates
    num_layers: 2           # number of hidden layers
    hidden_units: 64        # number of hidden unit in the hidden layer
    use_recurrent: false    # use R-NN
    lambd: 0.95
    learning_rate: 0.001    # learning rate or learning factor
    max_steps: 5000000      # maximum number of state for an episode
    normalize: false        # add normalization layer to the associated NN
    num_epoch: 10000        # total number of epochs
    time_horizon: 64        # time horizon
    sequence_length: 64     # 
    summary_freq: 1000      # Frequency at which at summary will be printed out

MCD_A2C:
    trainer: A2C           # trainer type
    batch_size: 512        # batch size
    memory_size: 512       # allocated memory size
    gamma: 0.99             # discount factor
    num_layers: 2           # number of hidden layers
    hidden_units: 64        # number of hidden unit in the hidden layer
    use_recurrent: false    # use R-NN
    learning_rate: 0.001    # learning rate or learning factor
    max_steps: 5000000      # maximum number of state for an episode
    normalize: false        # add normalization layer to the associated NN
    num_epoch: 10000        # total number of epochs
    summary_freq: 1000      # Frequency at which at summary will be printed out

MCD_MADDPG:
    trainer: MADDPG           # trainer type
    agent_config_path: C:\GitHub\Multi-Agent-RL-Energy-Management\OpenAIGym\MARL_config.yaml
    p_min: 0
    p_max: 100
    p_slope: 10
    batch_size: 512        # batch size
    memory_size: 10240      # allocated memory size
    beta: 5.0e-3
    alpha: 0.2              # learning factor
    alpha_min: 0.01         # learning factor min
    alpha_decay: 0.995      # learning factor decay
    epsilon: 0.3            # exploration rate
    epsilon_min: 0.1        # minimal exploration rate
    epsilon_decay: 0.999
    gamma: 0.99             # discount factor
    tau: .001                # soft target model updates
    num_layers: 2           # number of hidden layers
    hidden_units: 64        # number of hidden unit in the hidden layer
    use_recurrent: false    # use R-NN
    lambd: 0.95
    learning_rate: 0.001    # learning rate or learning factor
    max_steps: 5000000      # maximum number of state for an episode
    normalize: false        # add normalization layer to the associated NN
    num_epoch: 10000        # total number of epochs
    time_horizon: 64        # time horizon
    sequence_length: 64     # 
    summary_freq: 1000      # Frequency at which at summary will be printed out

MCD_MAPPO:
    trainer: MAPPO          # trainer type
    agent_config_path: C:\GitHub\Multi-Agent-RL-Energy-Management\OpenAIGym\MARL_config.yaml
    batch_size: 512        # batch size
    memory_size: 10240      # allocated memory size
    beta: 5.0e-3
    loss_clipping: 0.1      # Only implemented clipping for the surrogate loss, paper said it was best
    entropy_loss: 0.001     # Entropy loss
    exploration_noise: 0.2  # Exploration noise
    gamma: 0.99             # discount factor
    num_layers: 2           # number of hidden layers
    hidden_units: 64        # number of hidden unit in the hidden layer
    critic_hidden_units: 256 # number of hidden unit in the hidden layer
    use_recurrent: false    # use R-NN
    lambd: 0.95
    learning_rate: 0.001    # learning rate or learning factor
    max_steps: 5000000      # maximum number of state for an episode
    normalize: false        # add normalization layer to the associated NN
    num_epoch: 10000        # total number of epochs
    time_horizon: 64        # time horizon
    sequence_length: 64     # 
    summary_freq: 1000      # Frequency at which at summary will be printed out

MCD_MAPPO_mult:
    trainer: MAPPOv2          # trainer type ie. MAPPO, MAPPOv2, MAPPOv3
    agent_config_path: C:\GitHub\Multi-Agent-RL-Energy-Management\OpenAIGym\MARL_config.yaml
    batch_size: 256         # batch size. Normally 4 to 4096
    memory_size: 102400      # allocated memory size
    beta: 5.0e-3
    loss_clipping: 0.3      # Only implemented clipping for the surrogate loss, paper said it was best. 0.1, 0.2, 0.3
    entropy_loss: 0.005     # Entropy loss. 0 to 0.01
    exploration_noise: 0.2  # Exploration noise. Normally 0.2
    gamma: 0.9             # discount factor. 0.8 to 0.9997
    num_layers: 2           # number of hidden layers
    hidden_units: 256        # number of hidden unit in the hidden layer
    critic_hidden_units: 256 # number of hidden unit in the critic hidden layer
    use_recurrent: false    # use R-NN
    lambd: 0.95             # GAE smoothing parameter
    learning_rate: 0.001    # learning rate or learning factor. Normally 0.003 to 5e-6
    max_steps: 1000         # maximum number of state for an episode
    normalize: false        # add normalization layer to the associated NN
    num_epoch: 2            # total number of epochs. 3 to 30
    time_horizon: 1024      # time horizon. Normally 32 to 5000
    sequence_length: 64     #
    summary_freq: 1000      # Frequency at which at summary will be printed out

MCD_MAPPOLSTM_mult:
    trainer: MAPPOLSTM          # trainer type
    agent_config_path: C:\GitHub\Multi-Agent-RL-Energy-Management\OpenAIGym\MARL_config.yaml
    batch_size: 512        # batch size
    memory_size: 10240      # allocated memory size
    beta: 5.0e-3
    loss_clipping: 0.1      # Only implemented clipping for the surrogate loss, paper said it was best
    entropy_loss: 0.001     # Entropy loss
    exploration_noise: 0.2  # Exploration noise
    gamma: 0.99             # discount factor
    num_layers: 2           # number of hidden layers
    hidden_units: 128        # number of hidden unit in the hidden layer
    critic_hidden_units: 512 # number of hidden unit in the hidden layer
    use_recurrent: false    # use R-NN
    lambd: 0.95
    learning_rate: 0.001    # learning rate or learning factor
    max_steps: 500000      # maximum number of state for an episode
    normalize: false        # add normalization layer to the associated NN
    num_epoch: 1000        # total number of epochs
    time_horizon: 64        # time horizon
    sequence_length: 64     #
    summary_freq: 1000      # Frequency at which at summary will be printed out

MCD_MAPPO_mult_v2:
    trainer: MAPPOv2         # trainer type
    agent_config_path: C:\GitHub\Multi-Agent-RL-Energy-Management\OpenAIGym\MARL_config.yaml
    batch_size: 512         # batch size. Normally 4 to 4096
    memory_size: 10240      # allocated memory size
    beta: 5.0e-3
    loss_clipping: 0.1      # Only implemented clipping for the surrogate loss, paper said it was best. 0.1, 0.2, 0.3
    entropy_loss: 0.005     # Entropy loss. 0 to 0.01
    exploration_noise: 0.2  # Exploration noise. Normally 0.2
    gamma: 0.99             # discount factor. 0.8 to 0.9997
    num_layers: 2           # number of hidden layers
    hidden_units: 64        # number of hidden unit in the hidden layer
    critic_hidden_units: 256 # number of hidden unit in the critic hidden layer
    use_recurrent: false    # use R-NN
    lambd: 0.95
    learning_rate: 0.001    # learning rate or learning factor. Normally 0.003 to 5e-6
    max_steps: 2000         # maximum number of state for an episode
    normalize: false        # add normalization layer to the associated NN
    num_epoch: 3            # total number of epochs. 3 to 30
    time_horizon: 512        # time horizon. Normally 32 to 5000
    sequence_length: 64     #
    summary_freq: 1000      # Frequency at which at summary will be printed out

MCD_MAPPO_mult_v3:
    trainer: MAPPOv3         # trainer type
    agent_config_path: C:\GitHub\Multi-Agent-RL-Energy-Management\OpenAIGym\MARL_config.yaml
    batch_size: 64         # batch size. Normally 4 to 4096
    memory_size: 10240      # allocated memory size
    beta: 5.0e-3
    loss_clipping: 0.1      # Only implemented clipping for the surrogate loss, paper said it was best. 0.1, 0.2, 0.3
    entropy_loss: 0.005     # Entropy loss. 0 to 0.01
    exploration_noise: 0.2  # Exploration noise. Normally 0.2
    gamma: 0.99             # discount factor. 0.8 to 0.9997
    num_layers: 2           # number of hidden layers
    hidden_units: 64        # number of hidden unit in the hidden layer
    critic_hidden_units: 256 # number of hidden unit in the critic hidden layer
    use_recurrent: false    # use R-NN
    lambd: 0.95             # GAE smoothing parameter
    learning_rate: 0.001    # learning rate or learning factor. Normally 0.003 to 5e-6
    max_steps: 2000         # maximum number of state for an episode
    normalize: false        # add normalization layer to the associated NN
    num_epoch: 3            # total number of epochs. 3 to 30
    time_horizon: 512        # time horizon. Normally 32 to 5000
    sequence_length: 64     #
    summary_freq: 1000      # Frequency at which at summary will be printed out